---
title: "Practical ML Prediction Project"
author: "Deb"
date: "Sunday, May 23, 2015"
output: html_document
---
#Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: <http://groupware.les.inf.puc-rio.br/har> (see the section on the Weight Lifting Exercise Dataset). 
 
#Data Source
Training Data:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

Testing Data:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

Note: Reproducibility is not needed as part of security reason

##Setting up the Infrastructure:

```{r,echo=FALSE}
# Loading/installing the packages/libraries needed
# We need Caret,Rattle,Rpart,Applied predictive modelling, Random Forest

require(caret);require(rattle);require(AppliedPredictiveModeling);require(rpart.plot);require(randomForest);require(e1071)

```

## Data Preparation:

```{r}
# Pull the data from the source
# Empty, Div/0 is considered as NA
train_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train<-read.csv(url(train_url),na.strings=c("NA","#DIV/0!",""),header=TRUE)
test<-read.csv(url(test_url),na.strings=c("NA","#DIV/0!",""),header=TRUE)
#Compare the column names in both the data set
col_train<-colnames(train)
col_test<-colnames(test)
all.equal(col_train,col_test)
# Found one mismatch and will remove the column so that it doesnt impact the model
# Cleaning of the NA and other junk from the data
# Find number of non-NA's in the data
nonna<-function(x){
  as.vector(apply(x,2,function(x) length(which(!is.na(x)))))
}
#Vector of missing data
col<-nonna(train)

drops <- c()
for (cnt in 1:length(col)) {
    if (col[cnt] < nrow(train)) {
        drops <- c(drops, col_train[cnt])
    }
}
# Drop NA data and the first 7 columns, they are not needed for prediction
train <- train[,!(names(train) %in% drops)]
train <- train[,8:length(colnames(train))]

test <- test[,!(names(test) %in% drops)]
test <- test[,8:length(colnames(test))]

# Show remaining columns
colnames(train)
colnames(test)
#Check for near to zero variance
nzv<-nearZeroVar(train,saveMetrics=TRUE)
nzv
# All nearzerovar are False hence there is no need to eliminate covariates

```

## Modeling the data
```{r}
# As the training dataset size is quite big than test.Let's divide the train data into 2 parts with ratio of 60:40. And make 4 training set 

set.seed(777)
ids_small <- createDataPartition(y=train$classe, p=0.25, list=FALSE)
df_small1 <- train[ids_small,]
df_remainder <- train[-ids_small,]
set.seed(777)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.33, list=FALSE)
df_small2 <- df_remainder[ids_small,]
df_remainder <- df_remainder[-ids_small,]
set.seed(777)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.5, list=FALSE)
df_small3 <- df_remainder[ids_small,]
df_small4 <- df_remainder[-ids_small,]
# Divide each of these 4 sets into training (60%) and test (40%) sets.
set.seed(777)
inTrain <- createDataPartition(y=df_small1$classe, p=0.6, list=FALSE)
df_small_training1 <- df_small1[inTrain,]
df_small_testing1 <- df_small1[-inTrain,]
set.seed(777)
inTrain <- createDataPartition(y=df_small2$classe, p=0.6, list=FALSE)
df_small_training2 <- df_small2[inTrain,]
df_small_testing2 <- df_small2[-inTrain,]
set.seed(777)
inTrain <- createDataPartition(y=df_small3$classe, p=0.6, list=FALSE)
df_small_training3 <- df_small3[inTrain,]
df_small_testing3 <- df_small3[-inTrain,]
set.seed(777)
inTrain <- createDataPartition(y=df_small4$classe, p=0.6, list=FALSE)
df_small_training4 <- df_small4[inTrain,]
df_small_testing4 <- df_small4[-inTrain,]

#Classification Tree
# Train on training set 1 of 4 with no extra features.
set.seed(666)
modFit <- train(df_small_training1$classe ~ ., data = df_small_training1, method="rpart")
print(modFit, digits=3)
print(modFit$finalModel, digits=3)
#fancyRpartPlot(modFit$finalModel)
# Run against testing set 1 of 4 with no extra features.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)

#Preprocessing and crossvalidation
# Train on training set 1 of 4 with only preprocessing.
set.seed(777)
modFit <- train(df_small_training1$classe ~ .,  preProcess=c("center", "scale"), data = df_small_training1, method="rpart")
print(modFit, digits=3)

# Train on training set 1 of 4 with only cross validation.
set.seed(777)
modFit <- train(df_small_training1$classe ~ .,  trControl=trainControl(method = "cv", number = 4), data = df_small_training1, method="rpart")
print(modFit, digits=3)

# Train on training set 1 of 4 with both preprocessing and cross validation.
set.seed(777)
modFit <- train(df_small_training1$classe ~ .,  preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data = df_small_training1, method="rpart")
print(modFit, digits=3)

# Run against testing set 1 of 4 with both preprocessing and cross validation.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)

#Random Forest

# Train on training set 1 of 4 with only cross validation.
set.seed(777)
modFit <- train(df_small_training1$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=df_small_training1)
print(modFit, digits=3)

# Run against testing set 1 of 4.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)

# Run against 20 testing set provided
print(predict(modFit, newdata=test))

# Train on training set 1 of 4 with only both preprocessing and cross validation.
set.seed(777)
modFit <- train(df_small_training1$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training1)
print(modFit, digits=3)

# Run against testing set 1 of 4.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)


# Run against 20 testing set provided
print(predict(modFit, newdata=test))

# Preprocessing lowered the accuracy rate.When run against corresponding set accuracy rose with the addition of preprocessing. So applied both preprocessing and cross validation

# Train on training set 2 of 4 with only cross validation.
set.seed(666)
modFit <- train(df_small_training2$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training2)
print(modFit, digits=3)

# Run against testing set 2 of 4.
predictions <- predict(modFit, newdata=df_small_testing2)
print(confusionMatrix(predictions, df_small_testing2$classe), digits=4)

# Run against 20 testing set provided
print(predict(modFit, newdata=test))

# Train on training set 3 of 4 with only cross validation.
set.seed(666)
modFit <- train(df_small_training3$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training3)
print(modFit, digits=3)

# Run against testing set 3 of 4.
predictions <- predict(modFit, newdata=df_small_testing3)
print(confusionMatrix(predictions, df_small_testing3$classe), digits=4)

# Run against 20 testing set provided
print(predict(modFit, newdata=test))

# Train on training set 4 of 4 with only cross validation.
set.seed(666)
modFit <- train(df_small_training4$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training4)
print(modFit, digits=3)

# Run against testing set 4 of 4.
predictions <- predict(modFit, newdata=df_small_testing4)
print(confusionMatrix(predictions, df_small_testing4$classe), digits=4)

# Run against 20 testing set provided
print(predict(modFit, newdata=test))

```

## Out of sample error
The out of sample error is the error rate you get on new data set

Random Forest: Testing 1:1 Accuracy= .9714
Random Forest: Testing 1:2 Accuracy= .9634
Random Forest: Testing 1:3 Accuracy= .9655
Random Forest: Testing 1:4 Accuracy= .9563

# Resutlt
A) Accuracy Rate 0.9714 Predictions: B A A A A E D B A A B C B A E E A B B B

B) Accuracy Rates 0.9634 and 0.9655 Predictions: B A B A A E D B A A B C B A E E A B B B

C) Accuracy Rate 0.9563 Predictions: B A B A A E D D A A B C B A E E A B B B




